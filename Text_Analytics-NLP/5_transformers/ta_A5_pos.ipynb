{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics - Assignment 5\n",
    "\n",
    "Grammatikopoulou Maria - f3352310\n",
    "\n",
    "Phevos A. Margonis - f3352317\n",
    "\n",
    "Moniaki Melina - f3352321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP Parsing and Processing - Uncomment if using these libraries<br>\n",
    "# !pip install transformers<br>\n",
    "# !pip install datasets<br>\n",
    "# !pip install accelerate<br>\n",
    "# !pip install conllu<br>\n",
    "# !pip install halo<br>\n",
    "# !pip install pydot<br>\n",
    "# !pip install fasttext-wheel<br>\n",
    "# !pip install tensorflow[and-cuda] # For linux & GPU support<br>\n",
    "import gensim.downloader as api\n",
    "import pickle\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import nltk\n",
    "# import fasttext\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# from catalogue import create\n",
    "from conllu import parse_incr\n",
    "# from halo import Halo\n",
    "from keras.utils import to_categorical\n",
    "from numpy.random import default_rng\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, auc, classification_report,\n",
    "                             f1_score, precision_recall_curve, precision_score,\n",
    "                             recall_score)\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from tensorflow.keras.callbacks import (EarlyStopping, ModelCheckpoint,\n",
    "                                        TensorBoard)\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import transformers\n",
    "transformers.__version__\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = default_rng(587)\n",
    "#%% Reign-in VRAM use from Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% load and explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = pd.read_csv('en_gum_train.csv').drop_duplicates().reset_index(drop=True)\n",
    "valDF = pd.read_csv('en_gum_val.csv')\n",
    "testDF = pd.read_csv('en_gum_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8165 entries, 0 to 8164\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    8165 non-null   object\n",
      " 1   labels  8165 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 127.7+ KB\n"
     ]
    }
   ],
   "source": [
    "trainDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8165</td>\n",
       "      <td>8165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8160</td>\n",
       "      <td>7331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Sure .</td>\n",
       "      <td>INTJ PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text      labels\n",
       "count     8165        8165\n",
       "unique    8160        7331\n",
       "top     Sure .  INTJ PUNCT\n",
       "freq         3          61"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    8165.000000\n",
      "mean       18.003552\n",
      "std        13.221574\n",
      "min         1.000000\n",
      "25%         8.000000\n",
      "50%        15.000000\n",
      "75%        25.000000\n",
      "max       110.000000\n",
      "dtype: float64\n",
      "90th Percentile length of train sentences: 35.0\n"
     ]
    }
   ],
   "source": [
    "trainDF.head()\n",
    "print(pd.Series([len(text.split()) for text in trainDF.text]).describe())\n",
    "percentile = 90\n",
    "print(f\"{percentile}th Percentile length of train sentences: {np.percentile([len(text.split()) for text in trainDF.text],percentile)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aesthetic Appreciation and Spanish Art :\n",
      "ADJ NOUN CCONJ ADJ NOUN PUNCT\n"
     ]
    }
   ],
   "source": [
    "print(trainDF.text[0])\n",
    "print(trainDF.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Names: ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n"
     ]
    }
   ],
   "source": [
    "TARGET_NAMES = sorted(list(set(' '.join(trainDF.labels).split())))\n",
    "NCLASSES = len(TARGET_NAMES)\n",
    "print('Target Names:',TARGET_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% Majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_and_encode(df) -> pd.DataFrame:\n",
    "    '''Flatten each text sentence and each label. Then Encode the labels as category ints'''\n",
    "    df_flattened        = df['text'].str.lower().str.split(expand=True).stack().reset_index(drop=True).to_frame(name='word') # Flatten train data\n",
    "    df_flattened['tag'] = df['labels'].str.lower().str.split(expand=True).stack().reset_index(drop=True)\n",
    "    df_flattened['tag'] = df_flattened['tag'].astype('category').cat.codes # Encode 'tag'\n",
    "    return df_flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority Dummy classifier: Find the most common label for each unique word using the Train-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flattened  = flatten_and_encode(trainDF)\n",
    "trainMajorityTag = train_flattened.groupby('word')['tag'].agg(lambda x: x.mode()[0]).to_frame() # Find the most common tag for each word\n",
    "trainMajorityTag = trainMajorityTag.tag.to_dict() # dict{word: most_commont_tag}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten evaluation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_flattened  = flatten_and_encode(valDF)\n",
    "test_flattened = flatten_and_encode(testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get y_true for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_true = train_flattened.tag.tolist()\n",
    "y_val_true   = val_flattened.tag.tolist()\n",
    "y_test_true  = test_flattened.tag.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the overall most common tag inside train-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_tag = train_flattened.tag.mode()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the prediction using Majority dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = [trainMajorityTag.get(word,most_common_tag) for word in train_flattened.word]\n",
    "y_val_pred   = [trainMajorityTag.get(word,most_common_tag) for word in val_flattened.word]\n",
    "y_test_pred  = [trainMajorityTag.get(word,most_common_tag) for word in test_flattened.word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_val_acc = accuracy_score(y_val_true, y_val_pred).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.91      0.93      0.92      9761\n",
      "         ADP       0.90      0.89      0.90     14045\n",
      "         ADV       0.89      0.85      0.87      6969\n",
      "         AUX       0.87      0.95      0.91      7689\n",
      "       CCONJ       0.99      1.00      0.99      4846\n",
      "         DET       0.95      0.98      0.96     12066\n",
      "        INTJ       0.73      0.76      0.74      1144\n",
      "        NOUN       0.92      0.95      0.93     24618\n",
      "         NUM       0.95      1.00      0.98      2924\n",
      "        PART       0.69      0.90      0.78      3466\n",
      "        PRON       0.92      0.96      0.94     12059\n",
      "       PROPN       0.94      0.82      0.88      8934\n",
      "       PUNCT       0.99      1.00      1.00     20274\n",
      "       SCONJ       0.85      0.33      0.48      2324\n",
      "         SYM       0.95      0.68      0.79       231\n",
      "        VERB       0.93      0.89      0.91     15347\n",
      "           X       0.97      0.73      0.84       302\n",
      "\n",
      "    accuracy                           0.92    146999\n",
      "   macro avg       0.90      0.86      0.87    146999\n",
      "weighted avg       0.92      0.92      0.92    146999\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.90      0.81      0.85      1340\n",
      "         ADP       0.91      0.91      0.91      1889\n",
      "         ADV       0.87      0.80      0.84       926\n",
      "         AUX       0.88      0.96      0.92      1069\n",
      "       CCONJ       0.98      1.00      0.99       653\n",
      "         DET       0.95      0.98      0.97      1598\n",
      "        INTJ       0.75      0.74      0.74       204\n",
      "        NOUN       0.74      0.94      0.83      3381\n",
      "         NUM       0.95      0.86      0.91       329\n",
      "        PART       0.75      0.92      0.83       510\n",
      "        PRON       0.93      0.96      0.94      1807\n",
      "       PROPN       0.78      0.40      0.53       833\n",
      "       PUNCT       1.00      1.00      1.00      2606\n",
      "       SCONJ       0.87      0.36      0.51       332\n",
      "         SYM       1.00      0.36      0.53        14\n",
      "        VERB       0.92      0.77      0.84      2150\n",
      "           X       1.00      0.50      0.67        12\n",
      "\n",
      "    accuracy                           0.88     19653\n",
      "   macro avg       0.89      0.78      0.81     19653\n",
      "weighted avg       0.89      0.88      0.88     19653\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.88      0.80      0.84      1331\n",
      "         ADP       0.91      0.91      0.91      2053\n",
      "         ADV       0.89      0.77      0.82       916\n",
      "         AUX       0.87      0.94      0.91       949\n",
      "       CCONJ       0.98      1.00      0.99       700\n",
      "         DET       0.96      0.98      0.97      1720\n",
      "        INTJ       0.78      0.68      0.72       148\n",
      "        NOUN       0.68      0.93      0.78      3508\n",
      "         NUM       0.97      0.81      0.89       401\n",
      "        PART       0.69      0.90      0.78       420\n",
      "        PRON       0.92      0.97      0.94      1435\n",
      "       PROPN       0.79      0.35      0.49      1409\n",
      "       PUNCT       0.99      1.00      1.00      2540\n",
      "       SCONJ       0.88      0.38      0.54       252\n",
      "         SYM       0.82      0.53      0.64        34\n",
      "        VERB       0.90      0.76      0.83      2073\n",
      "           X       0.88      0.50      0.64        28\n",
      "\n",
      "    accuracy                           0.86     19917\n",
      "   macro avg       0.87      0.78      0.81     19917\n",
      "weighted avg       0.87      0.86      0.85     19917\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train_true, y_train_pred, target_names=TARGET_NAMES))\n",
    "print(classification_report(y_val_true, y_val_pred, target_names=TARGET_NAMES))\n",
    "print(classification_report(y_test_true, y_test_pred, target_names=TARGET_NAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "There is an issue with Tensorflow which takes up too much VRAM, so the following workaround must be made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize y<br>\n",
    "vectorize_Y = layers.TextVectorization(max_tokens=MAX_WORDS,<br>\n",
    "                                output_mode='int',<br>\n",
    "                                standardize=None,<br>\n",
    "                                ngrams=1,<br>\n",
    "                                vocabulary=TARGET_NAMES,<br>\n",
    "                                output_sequence_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_train_sequential = vectorize_Y(trainDF.labels.values).numpy()<br>\n",
    "y_val_sequential = vectorize_Y(valDF.labels.values).numpy()<br>\n",
    "y_test_sequential = vectorize_Y(testDF.labels.values).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the arrays<br>\n",
    "np.save('y_train_sequential.npy', y_train_sequential)<br>\n",
    "np.save('y_val_sequential.npy', y_val_sequential)<br>\n",
    "np.save('y_test_sequential.npy', y_test_sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the arrays back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sequential = np.load('y_train_sequential.npy')\n",
    "y_val_sequential = np.load('y_val_sequential.npy')\n",
    "y_test_sequential = np.load('y_test_sequential.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% Reformat input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset # Tropos na diaxeirizese ta data gia na ta dineis stous transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary containing X, y, for every subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'train':Dataset.from_dict({'label':y_train_sequential,'text':trainDF.text.str.split()}),\n",
    "     'val':Dataset.from_dict({'label':y_val_sequential,'text':valDF.text.str.split()}),\n",
    "     'test':Dataset.from_dict({'label':y_test_sequential,'text':testDF.text.str.split()})\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydataset = DatasetDict(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% Example train instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aesthetic', 'Appreciation', 'and', 'Spanish', 'Art', ':']\n",
      "[2, 9, 6, 2, 9, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(mydataset['train']['text'][0])\n",
    "print(mydataset['train']['label'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert/distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% Tokenization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 6249, 8925, 1011, 5811, 6249, 1012, 8925, 1011, 5811, 1030, 3417, 1012, 9353, 1012, 2866, 2118, 1997, 10913, 1010, 2142, 2983, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} \n",
      "\n",
      "[None, 0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 6, 7, 8, 9, 10, None] \n",
      "\n",
      "['[CLS]', 'claire', 'bailey', '-', 'ross', 'claire', '.', 'bailey', '-', 'ross', '@', 'port', '.', 'ac', '.', 'uk', 'university', 'of', 'portsmouth', ',', 'united', 'kingdom', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(mydataset[\"train\"][2][\"text\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokenized_input, '\\n')\n",
    "print(tokenized_input.word_ids(), '\\n')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 4372, 3900, 14905, 3672, 3138, 2173, 2043, 1037, 19962, 2696, 13306, 3131, 2003, 3714, 2039, 2408, 2048, 3210, 1997, 4623, 1006, 14383, 2075, 29488, 6178, 2906, 7352, 1010, 2456, 1024, 9800, 1007, 1010, 3228, 4125, 2000, 2367, 24828, 3896, 1006, 1041, 1012, 1043, 1012, 3445, 7902, 2006, 3787, 1997, 1996, 3714, 1011, 2039, 7655, 1010, 2030, 5688, 2090, 2216, 3787, 1007, 1010, 2030, 4526, 3313, 15931, 2005, 1996, 4372, 3900, 18552, 2094, 3210, 1006, 7439, 1011, 6643, 6460, 1010, 2889, 1007, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} \n",
      "\n",
      "[None, 0, 0, 0, 0, 1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 16, 16, 17, 17, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 31, 31, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 56, 56, 56, 57, 58, 59, 60, 61, 61, 62, 63, 64, 65, None] \n",
      "\n",
      "['[CLS]', 'en', '##ja', '##mb', '##ment', 'takes', 'place', 'when', 'a', 'syn', '##ta', '##ctic', 'unit', 'is', 'broken', 'up', 'across', 'two', 'lines', 'of', 'poetry', '(', 'dom', '##ing', '##uez', 'cap', '##ar', '##ros', ',', '2000', ':', '103', ')', ',', 'giving', 'rise', 'to', 'different', 'stylistic', 'effects', '(', 'e', '.', 'g', '.', 'increased', 'emphasis', 'on', 'elements', 'of', 'the', 'broken', '-', 'up', 'phrase', ',', 'or', 'contrast', 'between', 'those', 'elements', ')', ',', 'or', 'creating', 'double', 'interpretations', 'for', 'the', 'en', '##ja', '##mbe', '##d', 'lines', '(', 'garcia', '-', 'pa', '##je', ',', '1991', ')', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(mydataset[\"train\"][113][\"text\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokenized_input, '\\n')\n",
    "print(tokenized_input.word_ids(), '\\n')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% Re-align tokens after BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    global tokenizer\n",
    "    ''' \n",
    "    Adding the special tokens [CLS] and [SEP] and subword tokenization creates a mismatch between\n",
    "    the input and labels. A single word corresponding to a single label may be split into two subwords. \n",
    "    You will need to realign the tokens and labels by:\n",
    "        \n",
    "        1. Mapping all tokens to their corresponding word with the word_ids method.\n",
    "        2. Assigning the label -100 to the special tokens [CLS] and [SEP] so the loss function ignores them.\n",
    "        3. Only labeling the first token of a given word. Assign -100 to other subtokens from the same word.\n",
    "        \n",
    "    Here is how you can create a function to realign the tokens and labels, and truncate sequences to be no longer than model's maximum input length:\n",
    "    '''\n",
    "    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, \n",
    "                                 is_split_into_words=True, \n",
    "                                 add_special_tokens=True, \n",
    "                                 padding='max_length',\n",
    "                                 max_length=32, \n",
    "                                 return_tensors='pt'\n",
    "                                 )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"label\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1096 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myTokenizedDataset = mydataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': [13,\n",
       "  13,\n",
       "  14,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  3,\n",
       "  13,\n",
       "  14,\n",
       "  17,\n",
       "  13,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'text': ['Claire',\n",
       "  'Bailey',\n",
       "  '-',\n",
       "  'Ross',\n",
       "  'claire.bailey-ross@port.ac.uk',\n",
       "  'University',\n",
       "  'of',\n",
       "  'Portsmouth',\n",
       "  ',',\n",
       "  'United',\n",
       "  'Kingdom'],\n",
       " 'input_ids': [101,\n",
       "  6249,\n",
       "  8925,\n",
       "  1011,\n",
       "  5811,\n",
       "  6249,\n",
       "  1012,\n",
       "  8925,\n",
       "  1011,\n",
       "  5811,\n",
       "  1030,\n",
       "  3417,\n",
       "  1012,\n",
       "  9353,\n",
       "  1012,\n",
       "  2866,\n",
       "  2118,\n",
       "  1997,\n",
       "  10913,\n",
       "  1010,\n",
       "  2142,\n",
       "  2983,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [-100,\n",
       "  13,\n",
       "  13,\n",
       "  14,\n",
       "  13,\n",
       "  13,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  13,\n",
       "  3,\n",
       "  13,\n",
       "  14,\n",
       "  17,\n",
       "  13,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myTokenizedDataset['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=19).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids.flatten()  # Flatten the label_ids\n",
    "    preds = pred.predictions.argmax(-1).flatten()  # Flatten and get the max prediction\n",
    "    \n",
    "    mask = (labels != 0)\n",
    "    \n",
    "    # Apply mask\n",
    "    masked_labels = labels[mask]\n",
    "    masked_preds = preds[mask]\n",
    "\n",
    "    # Calculate metrics using sklearn's functions\n",
    "    acc = accuracy_score(masked_labels, masked_preds)\n",
    "    # f1 = f1_score(masked_labels, masked_preds, average='macro')\n",
    "\n",
    "    return {\n",
    "        'val_accuracy': acc,\n",
    "        # 'val_f1': f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2810' max='5100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2810/5100 08:00 < 06:32, 5.84 it/s, Epoch 10/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Val Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.984467</td>\n",
       "      <td>0.424117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.382900</td>\n",
       "      <td>0.486161</td>\n",
       "      <td>0.673382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.382900</td>\n",
       "      <td>0.317175</td>\n",
       "      <td>0.789215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.423100</td>\n",
       "      <td>0.255461</td>\n",
       "      <td>0.831485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.423100</td>\n",
       "      <td>0.223713</td>\n",
       "      <td>0.856722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.268400</td>\n",
       "      <td>0.204747</td>\n",
       "      <td>0.873076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.268400</td>\n",
       "      <td>0.188690</td>\n",
       "      <td>0.881168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.206600</td>\n",
       "      <td>0.178343</td>\n",
       "      <td>0.892429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.206600</td>\n",
       "      <td>0.180714</td>\n",
       "      <td>0.892202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.168300</td>\n",
       "      <td>0.170694</td>\n",
       "      <td>0.899049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.168300</td>\n",
       "      <td>0.166730</td>\n",
       "      <td>0.904086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2810, training_loss=0.45298437614033654, metrics={'train_runtime': 480.9244, 'train_samples_per_second': 339.554, 'train_steps_per_second': 10.605, 'total_flos': 733638448297920.0, 'train_loss': 0.45298437614033654, 'epoch': 11.0})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=20,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    warmup_steps=500,  # Adjust based on your setup\n",
    "    evaluation_strategy=\"epoch\",  # Evaluation at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Align saving strategy with evaluation strategy\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    gradient_accumulation_steps=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=myTokenizedDataset[\"train\"],\n",
    "    eval_dataset=myTokenizedDataset[\"val\"],\n",
    "    tokenizer=tokenizer,\n",
    "    # data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,                # the callback that computes metrics of interest\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01)]  # Add your early stopping callback here\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.97      0.91      0.94      8907\n",
      "         ADP       0.95      0.93      0.94     12922\n",
      "         ADV       0.96      0.92      0.94      6508\n",
      "         AUX       0.98      0.96      0.97      7266\n",
      "       CCONJ       0.97      0.92      0.94      4356\n",
      "         DET       0.98      0.96      0.97     11187\n",
      "        INTJ       0.96      0.88      0.92      1118\n",
      "        NOUN       0.90      0.97      0.93     22484\n",
      "         NUM       0.90      0.88      0.89      2567\n",
      "        PART       0.98      0.94      0.96      3253\n",
      "        PRON       0.97      0.96      0.97     11447\n",
      "       PROPN       0.91      0.92      0.91      8116\n",
      "       PUNCT       0.94      0.94      0.94     17766\n",
      "       SCONJ       0.94      0.90      0.92      2183\n",
      "         SYM       0.90      0.38      0.53       211\n",
      "        VERB       0.95      0.96      0.95     14428\n",
      "           X       0.89      0.51      0.65       265\n",
      "\n",
      "   micro avg       0.94      0.94      0.94    134984\n",
      "   macro avg       0.94      0.87      0.90    134984\n",
      "weighted avg       0.95      0.94      0.94    134984\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.92      0.86      0.89      1187\n",
      "         ADP       0.92      0.90      0.91      1711\n",
      "         ADV       0.93      0.86      0.90       848\n",
      "         AUX       0.96      0.94      0.95       991\n",
      "       CCONJ       0.95      0.89      0.92       558\n",
      "         DET       0.95      0.93      0.94      1450\n",
      "        INTJ       0.90      0.81      0.85       201\n",
      "        NOUN       0.86      0.93      0.89      3026\n",
      "         NUM       0.85      0.80      0.82       275\n",
      "        PART       0.96      0.92      0.94       468\n",
      "        PRON       0.95      0.95      0.95      1687\n",
      "       PROPN       0.81      0.78      0.80       746\n",
      "       PUNCT       0.92      0.92      0.92      2229\n",
      "       SCONJ       0.87      0.81      0.84       305\n",
      "         SYM       0.00      0.00      0.00         7\n",
      "        VERB       0.91      0.92      0.91      1972\n",
      "           X       1.00      0.09      0.17        11\n",
      "\n",
      "   micro avg       0.91      0.90      0.91     17672\n",
      "   macro avg       0.86      0.78      0.80     17672\n",
      "weighted avg       0.91      0.90      0.91     17672\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.90      0.83      0.87      1233\n",
      "         ADP       0.90      0.88      0.89      1917\n",
      "         ADV       0.89      0.85      0.87       861\n",
      "         AUX       0.95      0.95      0.95       900\n",
      "       CCONJ       0.92      0.87      0.90       627\n",
      "         DET       0.94      0.93      0.93      1633\n",
      "        INTJ       0.90      0.79      0.84       144\n",
      "        NOUN       0.84      0.90      0.87      3243\n",
      "         NUM       0.83      0.84      0.83       365\n",
      "        PART       0.93      0.92      0.93       393\n",
      "        PRON       0.95      0.94      0.94      1347\n",
      "       PROPN       0.79      0.75      0.77      1294\n",
      "       PUNCT       0.89      0.88      0.89      2236\n",
      "       SCONJ       0.86      0.85      0.86       241\n",
      "         SYM       0.89      0.25      0.39        32\n",
      "        VERB       0.93      0.90      0.91      1962\n",
      "           X       0.73      0.46      0.56        24\n",
      "\n",
      "   micro avg       0.89      0.88      0.89     18452\n",
      "   macro avg       0.88      0.81      0.84     18452\n",
      "weighted avg       0.89      0.88      0.89     18452\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Classification Report\n",
    "mask_train = (y_train_sequential.flatten() != 0)\n",
    "pred_train_probas = softmax(torch.tensor(trainer.predict(myTokenizedDataset[\"train\"]).predictions), dim=-1)\n",
    "pred_train_labels = np.argmax(pred_train_probas, axis=-1)\n",
    "print(classification_report(y_train_sequential.flatten()[mask_train], \n",
    "                            pred_train_labels.flatten()[mask_train],\n",
    "                            labels=list(range(2,19)),\n",
    "                            target_names=TARGET_NAMES\n",
    "                            ))\n",
    "# Validation Classification Report\n",
    "mask_val = (y_val_sequential.flatten() != 0)\n",
    "pred_val_probas = softmax(torch.tensor(trainer.predict(myTokenizedDataset[\"val\"]).predictions), dim=-1)\n",
    "pred_val_labels = np.argmax(pred_val_probas, axis=-1)\n",
    "print(classification_report(y_val_sequential.flatten()[mask_val], \n",
    "                            pred_val_labels.flatten()[mask_val],\n",
    "                            labels=list(range(2,19)),\n",
    "                            target_names=TARGET_NAMES\n",
    "                            ))\n",
    "\n",
    "# Test Classification Report\n",
    "mask_test = (y_test_sequential.flatten() != 0)\n",
    "pred_test_probas = softmax(torch.tensor(trainer.predict(myTokenizedDataset[\"test\"]).predictions), dim=-1)\n",
    "pred_test_labels = np.argmax(pred_test_probas, axis=-1)\n",
    "print(classification_report(y_test_sequential.flatten()[mask_test], \n",
    "                            pred_test_labels.flatten()[mask_test],\n",
    "                            labels=list(range(2,19)),\n",
    "                            target_names=TARGET_NAMES\n",
    "                            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class probabilities to be used for PR-AUC\n",
    "def my_padded_multiclass_auc_report( y_true, y_pred, title: str, TARGET_NAMES=TARGET_NAMES) -> None:\n",
    "    \"\"\"\n",
    "    Calculate the Precision-Recall AUC for each class, and then their Macro-averaged score.\n",
    "    The classes are padded with '', and [UNK] classes because they have beed created with \n",
    "    TextVectorizer() layer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.array of int\n",
    "        The one-hot label vectors for each data-point.\n",
    "    y_pred : np.array of float\n",
    "        The MxN predicted probabilities for each data-point (M) and class (N).\n",
    "    title : str\n",
    "        The title of the data-set that is processed. E.g.: Train/Val/Test.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"=== PR-AUC for {title} ===\")\n",
    "    macro_auc = []\n",
    "    for i in range(2,len(TARGET_NAMES)):\n",
    "        precision, recall, _ = precision_recall_curve(y_true[:, i], y_pred[:, i])\n",
    "        area = auc(recall, precision)\n",
    "        print(f\"PR-AUC for class {TARGET_NAMES[i]}: {area*100:.2f}%\")\n",
    "        macro_auc.append(area)\n",
    "\n",
    "    print(f\"-> Macro-averaged PR-AUC: {np.mean(macro_auc)*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PR-AUC for Train ===\n",
      "PR-AUC for class ADV: 98.00%\n",
      "PR-AUC for class AUX: 98.65%\n",
      "PR-AUC for class CCONJ: 97.69%\n",
      "PR-AUC for class DET: 99.07%\n",
      "PR-AUC for class INTJ: 97.63%\n",
      "PR-AUC for class NOUN: 99.18%\n",
      "PR-AUC for class NUM: 97.27%\n",
      "PR-AUC for class PART: 99.15%\n",
      "PR-AUC for class PRON: 95.36%\n",
      "PR-AUC for class PROPN: 98.01%\n",
      "PR-AUC for class PUNCT: 99.43%\n",
      "PR-AUC for class SCONJ: 97.58%\n",
      "PR-AUC for class SYM: 99.11%\n",
      "PR-AUC for class VERB: 95.64%\n",
      "PR-AUC for class X: 65.69%\n",
      "-> Macro-averaged PR-AUC: 95.83%\n",
      "\n",
      "=== PR-AUC for Validation ===\n",
      "PR-AUC for class ADV: 94.20%\n",
      "PR-AUC for class AUX: 96.92%\n",
      "PR-AUC for class CCONJ: 94.34%\n",
      "PR-AUC for class DET: 98.35%\n",
      "PR-AUC for class INTJ: 95.62%\n",
      "PR-AUC for class NOUN: 98.23%\n",
      "PR-AUC for class NUM: 93.01%\n",
      "PR-AUC for class PART: 96.78%\n",
      "PR-AUC for class PRON: 91.02%\n",
      "PR-AUC for class PROPN: 97.01%\n",
      "PR-AUC for class PUNCT: 98.82%\n",
      "PR-AUC for class SCONJ: 87.58%\n",
      "PR-AUC for class SYM: 98.24%\n",
      "PR-AUC for class VERB: 88.67%\n",
      "PR-AUC for class X: 3.02%\n",
      "-> Macro-averaged PR-AUC: 88.79%\n",
      "\n",
      "=== PR-AUC for Test ===\n",
      "PR-AUC for class ADV: 92.33%\n",
      "PR-AUC for class AUX: 95.60%\n",
      "PR-AUC for class CCONJ: 93.09%\n",
      "PR-AUC for class DET: 98.31%\n",
      "PR-AUC for class INTJ: 94.73%\n",
      "PR-AUC for class NOUN: 97.29%\n",
      "PR-AUC for class NUM: 93.03%\n",
      "PR-AUC for class PART: 95.19%\n",
      "PR-AUC for class PRON: 90.89%\n",
      "PR-AUC for class PROPN: 96.48%\n",
      "PR-AUC for class PUNCT: 98.22%\n",
      "PR-AUC for class SCONJ: 85.05%\n",
      "PR-AUC for class SYM: 96.75%\n",
      "PR-AUC for class VERB: 92.24%\n",
      "PR-AUC for class X: 46.16%\n",
      "-> Macro-averaged PR-AUC: 91.02%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Suppress specific warnings temporarily\n",
    "warnings.filterwarnings(\"ignore\", message=\"No positive class found in y_true\")\n",
    "\n",
    "# Predict probas.\n",
    "# Reshape 3D to 2D (samples, max_sequence, classes) -> (tokens,classes). Then apply mask.\n",
    "predictions_train = pred_train_probas\n",
    "predictions_train = predictions_train.reshape(-1, predictions_train.shape[-1])[mask_train]\n",
    "predictions_val   = pred_val_probas\n",
    "predictions_val   = predictions_val.reshape(-1, predictions_val.shape[-1])[mask_val]\n",
    "predictions_test  = pred_test_probas\n",
    "predictions_test  = predictions_test.reshape(-1, predictions_test.shape[-1])[mask_test]\n",
    "\n",
    "# PR-AUC report\n",
    "my_padded_multiclass_auc_report(to_categorical(y_train_sequential.flatten()[mask_train]), predictions_train, 'Train')\n",
    "my_padded_multiclass_auc_report(to_categorical(y_val_sequential.flatten()[mask_val]), predictions_val, 'Validation')\n",
    "my_padded_multiclass_auc_report(to_categorical(y_test_sequential.flatten()[mask_test]), predictions_test, 'Test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
