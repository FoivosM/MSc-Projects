{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMGmK5LMAoSG"
      },
      "source": [
        "# Text Analytics - Assignment 1\n",
        "\n",
        "Grammatikopoulou Maria - f3352310\n",
        "\n",
        "Phevos A. Margonis - f3352317\n",
        "\n",
        "Moniaki Melina - f3352321"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUgvjH7YAoSI"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import random\n",
        "import math\n",
        "import copy\n",
        "import nltk\n",
        "import pprint\n",
        "import Levenshtein\n",
        "from evaluate import load\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import brown\n",
        "from nltk import WhitespaceTokenizer\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from itertools import pairwise, chain, product\n",
        "from more_itertools import windowed\n",
        "\n",
        "# Uncomment the lines below for downloading NLTK resources if you haven't already.\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('brown')\n",
        "\n",
        "N = 10\n",
        "alpha = 0.01  # Smoothing hyperparameter. For Laplace 1. Initial 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCmHDzMNAoSJ"
      },
      "outputs": [],
      "source": [
        "# %% Load nltk corpus\n",
        "# Load all words from Brown Corpus.\n",
        "all_words = brown.words()\n",
        "text = ' '.join(all_words)\n",
        "# Load a specific category from the Brown Corpus.\n",
        "# category = 'news'\n",
        "# text = ' '.join(brown.words(categories=category))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnHyA7yTAoSL"
      },
      "outputs": [],
      "source": [
        "# %% Split corpus to sentences\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "sentences = [s[:-1] if s.endswith('.') else s for s in sentences]  # Remove trailing fullstops\n",
        "\n",
        "# %% Split each sentence of the corpus into words\n",
        "whitespace_wt = WhitespaceTokenizer()\n",
        "sentences_tokenized = []\n",
        "\n",
        "for sent in sentences:\n",
        "    sent_tok = whitespace_wt.tokenize(sent)\n",
        "    sent_tok = [word.lower() for word in sent_tok] # Convert word to lowercase\n",
        "    sentences_tokenized.append(sent_tok)\n",
        "\n",
        "# %% Train-Dev-Test Split\n",
        "random.seed(4444) # Set a seed for reproducibility\n",
        "random.shuffle(sentences_tokenized)\n",
        "\n",
        "# Calculate the lengths of each part\n",
        "total_len = len(sentences_tokenized)\n",
        "trainSet_len = int(total_len * 0.8)\n",
        "devSet_len = int(total_len * 0.1)\n",
        "testSet_len = int(total_len * 0.1)\n",
        "\n",
        "# Split the list into three parts\n",
        "trainSet = sentences_tokenized[:trainSet_len]\n",
        "devSet = sentences_tokenized[trainSet_len:trainSet_len + devSet_len]\n",
        "testSet = sentences_tokenized[trainSet_len + devSet_len:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REzvHhYSAoSM"
      },
      "outputs": [],
      "source": [
        "# %% Vocabulary: For the trainSet\n",
        "trainSet_words = list(chain(*trainSet))  # List of all the words in trainSet\n",
        "word_freq = nltk.FreqDist(trainSet_words)  # Counter of word frequencies\n",
        "vocab_words = [word for word, freq in word_freq.items() if freq > 9]  # Words that appear at least 10 times\n",
        "vocab_size = len(vocab_words)  # Count of the unique words of trainSet\n",
        "\n",
        "# %% Replace OOV with UNK\n",
        "for i, sentence in enumerate(trainSet):\n",
        "    trainSet[i] = [\"UNK\" if word not in vocab_words else word for word in sentence]\n",
        "for i, sentence in enumerate(devSet):\n",
        "    devSet[i] = [\"UNK\" if word not in vocab_words else word for word in sentence]\n",
        "for i, sentence in enumerate(testSet):\n",
        "    testSet[i] = [\"UNK\" if word not in vocab_words else word for word in sentence]\n",
        "\n",
        "# %% Vocabulary: Add 'UNK' because it is needed for (vi) when i have to correct spellcheck 'UNK' words\n",
        "vocab_words = vocab_words + ['UNK']\n",
        "vocab_size += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL21lazoAoSN"
      },
      "outputs": [],
      "source": [
        "# %% TRAINING phase: count n-grams\n",
        "unigram_counter = Counter()\n",
        "bigram_counter = Counter()\n",
        "trigram_counter = Counter()\n",
        "\n",
        "for sent in trainSet:\n",
        "    unigram_counter.update([gram for gram in ngrams(sent, 1, pad_left=True, pad_right=True,\n",
        "                                                    left_pad_symbol='<s>', right_pad_symbol='<e>')])\n",
        "    bigram_counter.update([gram for gram in ngrams(sent, 2, pad_left=True, pad_right=True,\n",
        "                                                   left_pad_symbol='<s>', right_pad_symbol='<e>')])\n",
        "    trigram_counter.update([gram for gram in ngrams(sent, 3, pad_left=True, pad_right=True,\n",
        "                                                    left_pad_symbol='<s>', right_pad_symbol='<e>')])\n",
        "\n",
        "# %% Convert ngram counts to ngram log probabilities\n",
        "bigram_model = {}  # Dictionary to store the bigram log probabilities\n",
        "\n",
        "for ngram, count in bigram_counter.items():\n",
        "    first_token, second_token = ngram\n",
        "    bigram_prob = (bigram_counter[(first_token, second_token)] + alpha) / (unigram_counter[(first_token,)] + alpha*vocab_size)  # P(w2|w1)\n",
        "    bigram_log_prob = math.log2(bigram_prob)\n",
        "    bigram_model[ngram] = bigram_log_prob\n",
        "\n",
        "trigram_model = {}  # Dictionary to store the trigram log probabilities\n",
        "\n",
        "for ngram, count in trigram_counter.items():\n",
        "    first_token, second_token, third_token = ngram\n",
        "    trigram_prob = (trigram_counter[(first_token, second_token, third_token)] + alpha) / (bigram_counter[(first_token, second_token)] + alpha * vocab_size)\n",
        "    trigram_log_prob = math.log2(trigram_prob)\n",
        "    trigram_model[ngram] = trigram_log_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "En8LABiGAoSP",
        "outputId": "4d37ddc7-dd3e-4c81-8d4d-ca0b6a20a9e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Bigram Model ===\n",
            "Cross Entropy: 7.322\n",
            "perplexity: 160.059\n"
          ]
        }
      ],
      "source": [
        "# %% Calculate LM Cross entropy & perplexity\n",
        "sum_prob = 0  # Store sum of language probabilities\n",
        "bigram_cnt = 0  # N: Number of bigrams\n",
        "\n",
        "for sent in testSet:\n",
        "    sent = ['<s>'] + sent + ['<e>']\n",
        "\n",
        "    # Iterate over the bigrmas of the sentence\n",
        "    for first_token, second_token in pairwise(sent):\n",
        "        bigram_prob = (bigram_counter[(first_token, second_token)] + alpha) / (unigram_counter[(first_token,)] + alpha*vocab_size)\n",
        "        sum_prob += math.log2(bigram_prob)\n",
        "        bigram_cnt += 1\n",
        "\n",
        "HC = -sum_prob / bigram_cnt\n",
        "perpl = math.pow(2, HC)\n",
        "print('=== Bigram Model ===')\n",
        "print(\"Cross Entropy: {0:.3f}\".format(HC))\n",
        "print(\"perplexity: {0:.3f}\".format(perpl))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Qwi_VJlAoSQ",
        "outputId": "5f0394e0-e0a4-4b36-c547-384958a7c118"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Trigram Model ===\n",
            "Cross Entropy: 9.252\n",
            "perplexity: 609.508\n"
          ]
        }
      ],
      "source": [
        "# %% Tri-gram LM Cross entropy & perplexity\n",
        "sum_prob = 0  # Store sum of language probabilities\n",
        "trigram_cnt = 0  # N: Number of trigrams\n",
        "\n",
        "for sent in testSet:\n",
        "    sent = ['<s>'] + ['<s>'] + sent + ['<e>']\n",
        "\n",
        "    for first_token, second_token, third_token in windowed(sent, n=3):\n",
        "        trigram_prob = (trigram_counter[(first_token, second_token, third_token)] + alpha) / (bigram_counter[(first_token, second_token)] + alpha*vocab_size)\n",
        "        sum_prob += math.log2(trigram_prob)\n",
        "        trigram_cnt += 1\n",
        "\n",
        "HC = -sum_prob / trigram_cnt\n",
        "perpl = math.pow(2, HC)\n",
        "print('=== Trigram Model ===')\n",
        "print(\"Cross Entropy: {0:.3f}\".format(HC))\n",
        "print(\"perplexity: {0:.3f}\".format(perpl))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7atxjCpcAoSR",
        "outputId": "ec939c29-e33f-4a18-c9d8-7b3c106d7a6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Autocompleted: 'I can't believe this is a few years ago'\n"
          ]
        }
      ],
      "source": [
        "# %% ===Bigram=== Beam search decoding\n",
        "def autocomplete_bigram(input_text:str, max_depth:int=1, beam_width:int=2) -> str:\n",
        "    \"\"\"\n",
        "    Takes half a sentence. Returns the completed sentence, using a BIGRAM LM.\n",
        "\n",
        "    The sentence will be tokenized, and the last word will be used as seed.\n",
        "    A beam search decoder will predict the most probable next words.\n",
        "    The predicted sentence will be concatenated with the input_text.\n",
        "\n",
        "    Args:\n",
        "        input_text: A string sentence that will be completed.\n",
        "        max_depth: The number of words to predict.\n",
        "        beam_width: The number of beams/best alternatives to keep.\n",
        "\n",
        "    Returns:\n",
        "        str: The input + predicted sentence.\n",
        "    \"\"\"\n",
        "    def generate_candidates(state:list[str]) -> list[list[str]]:\n",
        "        \"\"\"Given a sentence, generate possible next words (excluding UNK)\"\"\"\n",
        "        last_word = state[-1]\n",
        "        next_words = [word for (prev_word, word) in bigram_model if prev_word == last_word and word != 'UNK']\n",
        "        return [state + [next_word] for next_word in next_words]\n",
        "\n",
        "    def score(state:list[str]) -> float:\n",
        "        \"\"\"Return the probability assigned to this state by the bigram_model\"\"\"\n",
        "        probability = 0.0\n",
        "        for i in range(1, len(state)):\n",
        "            prev_word, word = state[i-1], state[i]\n",
        "            probability += bigram_model.get((prev_word, word), math.log2(1e-10))  # Σ[logP(<w1,w2,w3,...)]\n",
        "        return probability\n",
        "\n",
        "    def beam_search_decode(initial_state:list[str], max_depth:int, beam_width:int, generate_candidates_fn, score_fn) -> list[str]:\n",
        "        \"\"\" Takes a word (initial_state), and returns the most probable sentence. \"\"\"\n",
        "        candidates = [(initial_state, 0.0)]\n",
        "\n",
        "        for _ in range(max_depth):\n",
        "            new_candidates = []\n",
        "            for candidate, prob in candidates:\n",
        "                for next_state in generate_candidates_fn(candidate):\n",
        "                    new_prob = prob + score_fn(next_state)\n",
        "                    new_candidates.append((next_state, new_prob))\n",
        "\n",
        "            # Sort candidates. Best first.\n",
        "            new_candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n",
        "            # If there are no generated candidates from that bigram stop the autocomplete.\n",
        "            if not new_candidates:\n",
        "                candidates = [(list(candidates[0][0]) + ['<e>'], candidates[0][1])]\n",
        "                break\n",
        "            # Keep the top 2 candidates.\n",
        "            candidates = new_candidates[:beam_width]\n",
        "\n",
        "        best_sequence, _ = max(candidates, key=lambda x: x[1])\n",
        "        return best_sequence\n",
        "\n",
        "    initial_state = whitespace_wt.tokenize(input_text)\n",
        "    initial_state = [initial_state[-1]]\n",
        "\n",
        "    best_sequence = beam_search_decode(initial_state, max_depth, beam_width, generate_candidates, score)\n",
        "\n",
        "    # Join the input text and the best sequence with spaces\n",
        "    completed_sentence = ' '.join([input_text] + best_sequence[1:])\n",
        "    return completed_sentence\n",
        "\n",
        "# Example usage:\n",
        "input_text = \"I can't believe this\"\n",
        "autocompleted_sentence = autocomplete_bigram(input_text, max_depth=5, beam_width=2)\n",
        "print(f\"Autocompleted: '{autocompleted_sentence}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9exAa0QEAoST",
        "outputId": "041e9e1c-ac4f-4cad-935e-9ced2ca5a359"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Autocompleted: 'I can't believe this is a matter of fact , the'\n"
          ]
        }
      ],
      "source": [
        "# %% ===Trigram Autocomplete===\n",
        "def autocomplete_trigram(input_text:str, max_depth:int=1, beam_width:int=2) -> str:\n",
        "    \"\"\"\n",
        "    Takes half a sentence. Returns the completed sentence, using a TRIGRAM LM.\n",
        "\n",
        "    The sentence will be tokenized, and the last word will be used as seed.\n",
        "    A beam search decoder will predict the most probable next words.\n",
        "    The predicted sentence will be concatenated with the input_text.\n",
        "\n",
        "    Args:\n",
        "        input_text: A string sentence that will be completed.\n",
        "        max_depth: The number of words to predict.\n",
        "        beam_width: The number of beams/best alternatives to keep.\n",
        "\n",
        "    Returns:\n",
        "        str: The input + predicted sentence.\n",
        "    \"\"\"\n",
        "    def generate_candidates_tri(state:list[str]) -> list[list[str]]:\n",
        "        \"\"\"Given a sentence, generate possible next words (excluding UNK)\"\"\"\n",
        "        last_word1, last_word2 = state[-2], state[-1]\n",
        "        next_words = [word for (prev_word1, prev_word2, word) in trigram_model if prev_word1 == last_word1 and prev_word2 == last_word2 and word != 'UNK']\n",
        "        return [state + [next_word] for next_word in next_words]\n",
        "\n",
        "    def score_tri(state:list[str]) -> float:\n",
        "        \"\"\"Return the probability assigned to this state by the trigram_model\"\"\"\n",
        "        probability = 0.0\n",
        "        for i in range(2, len(state)):\n",
        "            prev_word1, prev_word2, word = state[i-2], state[i-1], state[i]\n",
        "            probability += trigram_model.get((prev_word1, prev_word2, word), math.log2(1e-10))  # Σ[logP(<w1,w2,w3,...)]\n",
        "        return probability\n",
        "\n",
        "    def beam_search_decode_tri(initial_state:list[str], max_depth:int, beam_width:int, generate_candidates_fn, score_fn) -> list[str]:\n",
        "        \"\"\" Takes a word (initial_state), and returns the most probable sentence\"\"\"\n",
        "        candidates = [(initial_state, 0.0)]\n",
        "\n",
        "        for _ in range(max_depth):\n",
        "            new_candidates = []\n",
        "            for candidate, prob in candidates:\n",
        "                for next_state in generate_candidates_fn(candidate):\n",
        "                    new_prob = prob + score_fn(next_state)\n",
        "                    new_candidates.append((next_state, new_prob))\n",
        "\n",
        "            # Sort candidates. Best first.\n",
        "            new_candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n",
        "            # If there are no generated candidates from that trigram stop the autocomplete\n",
        "            if not new_candidates:\n",
        "                candidates = [(list(candidates[0][0]) + ['<e>'], candidates[0][1])]\n",
        "                break\n",
        "            # Keep the top 2 candidates.\n",
        "            candidates = new_candidates[:beam_width]\n",
        "\n",
        "        best_sequence, _ = max(candidates, key=lambda x: x[1])\n",
        "        return best_sequence\n",
        "\n",
        "    initial_state = whitespace_wt.tokenize(input_text)\n",
        "    initial_state = initial_state[-2:]\n",
        "\n",
        "    best_sequence = beam_search_decode_tri(initial_state, max_depth, beam_width, generate_candidates_tri, score_tri)\n",
        "\n",
        "    # Join the input text and the best sequence with spaces\n",
        "    completed_sentence = ' '.join([input_text] + best_sequence[2:])  # 2 to hide the seed words\n",
        "    return completed_sentence\n",
        "\n",
        "# Example usage:\n",
        "input_text = \"I can't believe this is\"\n",
        "autocompleted_sentence = autocomplete_trigram(input_text, max_depth=6, beam_width=2)\n",
        "print(f\"Autocompleted: '{autocompleted_sentence}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgEe41y4AoSU",
        "outputId": "e965c9d1-53bb-4903-d3ed-0d0142d098d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['this', 'is', 'a', 'new', 'york', 'city']\n"
          ]
        }
      ],
      "source": [
        "# %% (iv) Bigram Spellcheck\n",
        "def spellcheck_bigram(word_list:list[str], beam_width:int=2) -> list[str]:\n",
        "    \"\"\"\n",
        "    Context-aware spelling-corrector.\n",
        "\n",
        "    Takes a list of words with spelling errors and\n",
        "    Returns a context-aware spell-corrected sentence as a list\n",
        "    Using a Bigram LM model and Levenshtein edit distance.\n",
        "\n",
        "        Args:\n",
        "            word_list:  the sentence to be corrected as a list of tokenized words\n",
        "            beam_width (int): the number of beams / best candidates to check\n",
        "\n",
        "        Returns:\n",
        "            List[str]: The spell-corrected list\n",
        "    \"\"\"\n",
        "    def generate_candidates(state:list[str]) -> list[list[str]]:\n",
        "        \"\"\"Given a sentence, generate possible next words\"\"\"\n",
        "        last_word = state[-1]\n",
        "        next_words = [word for (prev_word, word) in bigram_model if prev_word == last_word]\n",
        "        return [state + [next_word] for next_word in next_words]\n",
        "\n",
        "    def score(state:list[str], word_list:list[str], distances:dict[tuple,int]) -> float:\n",
        "        \"\"\"\n",
        "        Return the probability assigned to this state by the bigram_model\n",
        "        and the Levenshtein edit distance.\n",
        "        \"\"\"\n",
        "        probability = 0.0\n",
        "        for i in range(1, len(state)):\n",
        "            prev_word, word = state[i-1], state[i]\n",
        "            LM_proba = bigram_model.get((prev_word, word), math.log2(1e-10))\n",
        "            dist_check = distances.get((word_list[i-1], word), 1e10)\n",
        "            Edit_dist = math.log2(1/(dist_check + 1))\n",
        "            probability += 0.2 * LM_proba + 0.8 * Edit_dist\n",
        "        return probability\n",
        "\n",
        "    def beam_search_decode(initial_state:list[str], max_depth:int, beam_width:int, generate_candidates_fn, score_fn, word_list, distances) -> list[str]:\n",
        "        \"\"\" Takes a word (initial_state), and returns the most probable sentence. \"\"\"\n",
        "        candidates = [(initial_state, 0.0)]\n",
        "\n",
        "        for _ in range(max_depth):\n",
        "            new_candidates = []\n",
        "            for candidate, prob in candidates:\n",
        "                for next_state in generate_candidates_fn(candidate):\n",
        "                    new_prob = prob + score_fn(next_state, word_list, distances)\n",
        "                    new_candidates.append((next_state, new_prob))\n",
        "\n",
        "            # Sort candidates. Best first.\n",
        "            new_candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n",
        "            # If there are no generated candidates from that bigram stop the autocomplete.\n",
        "            if not new_candidates:\n",
        "                candidates = [(list(candidates[0][0]) + ['<e>'], candidates[0][1])]\n",
        "                break\n",
        "            # Keep the top 2 candidates.\n",
        "            candidates = new_candidates[:beam_width]\n",
        "\n",
        "        best_sequence, _ = max(candidates, key=lambda x: x[1])\n",
        "        return best_sequence\n",
        "\n",
        "    Vocabulary = vocab_words\n",
        "    distances = {(word, voc_token): Levenshtein.distance(word, voc_token) for word, voc_token in product(word_list, Vocabulary)}\n",
        "    initial_state = ['<s>']  # To be able to correct even the first input word.\n",
        "    max_depth = len(word_list)\n",
        "\n",
        "    best_sequence = beam_search_decode(initial_state, max_depth, beam_width, generate_candidates, score, word_list, distances)\n",
        "\n",
        "    return best_sequence[1:]  # Excluding the \"<start>\" token\n",
        "\n",
        "\n",
        "# Example use:\n",
        "inputText = ['thes', 'is', 'a', 'now', 'yrk', 'citi']  # word_list to be spell-corrected\n",
        "print(spellcheck_bigram(inputText))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EM4untTAoSV",
        "outputId": "236f0257-89bb-4c7c-eff7-eb2e28db129c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['this', 'is', 'a', 'new', 'york', 'city']\n"
          ]
        }
      ],
      "source": [
        "# %% (iv) TRIGRAM spellcheck\n",
        "def spellcheck_trigram(word_list:list[str], beam_width=2) -> list[str]:\n",
        "\n",
        "    \"\"\"\n",
        "    Takes a list of words with spelling errors and\n",
        "    returns a context-aware spell-corrected sentence as a list\n",
        "    using the trigram LM.\n",
        "\n",
        "        Parameters:\n",
        "        - word_list (str):  the sentence to be corrected as a list of tokenized words\n",
        "        - beam_width (int): the number of beams / best candidates to check\n",
        "\n",
        "        Returns:\n",
        "        - List[str]: The spell-corrected list\n",
        "    \"\"\"\n",
        "\n",
        "    def generate_candidates_tri(state:list[str]) -> list[list[str]]:\n",
        "        \"\"\"Given a sentence, generate possible next words\"\"\"\n",
        "        last_word1, last_word2 = state[-2], state[-1]\n",
        "        next_words = [word for (prev_word1, prev_word2, word) in trigram_model if prev_word1 == last_word1 and prev_word2 == last_word2]\n",
        "        return [state + [next_word] for next_word in next_words]\n",
        "\n",
        "    def score_tri(state:list[str], word_list:list[str], distances:dict[tuple,int]) -> float:\n",
        "        \"\"\"\n",
        "        Return the probability assigned to this state by the Trigram_model\n",
        "        and the Levenshtein edit distance.\n",
        "        \"\"\"\n",
        "        probability = 0.0\n",
        "        for i in range(2, len(state)):\n",
        "            prev_word1, prev_word2, word = state[i-2], state[i-1], state[i]\n",
        "            LM_proba = trigram_model.get((prev_word1, prev_word2, word), math.log2(1e-10))\n",
        "            dist_check = distances.get((word_list[i-2], word), 1e10)\n",
        "            Edit_dist = math.log2(1/(dist_check + 1))\n",
        "            probability += 0.2 * LM_proba + 0.8 * Edit_dist\n",
        "        return probability\n",
        "\n",
        "    def beam_search_decode_tri(initial_state:list[str],\n",
        "                               max_depth:int,\n",
        "                               beam_width:int,\n",
        "                               generate_candidates_fn,\n",
        "                               score_fn,\n",
        "                               word_list:list[str],\n",
        "                               distances:dict[tuple,int]) -> list[str]:\n",
        "        \"\"\" Takes a word (initial_state), and returns the most probable sentence\"\"\"\n",
        "        candidates = [(initial_state, 0.0)]\n",
        "\n",
        "        for _ in range(max_depth):\n",
        "            new_candidates = []\n",
        "            for candidate, prob in candidates:\n",
        "                for next_state in generate_candidates_fn(candidate):\n",
        "                    new_prob = prob + score_fn(next_state, word_list, distances)\n",
        "                    new_candidates.append((next_state, new_prob))\n",
        "\n",
        "            # Sort candidates. Best first.\n",
        "            new_candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n",
        "            # If there are no generated candidates from that trigram stop the autocomplete\n",
        "            if not new_candidates:\n",
        "                candidates = [(list(candidates[0][0]) + ['<e>'], candidates[0][1])]\n",
        "                break\n",
        "            # Keep the top 2 candidates.\n",
        "            candidates = new_candidates[:beam_width]\n",
        "\n",
        "        best_sequence, _ = max(candidates, key=lambda x: x[1])\n",
        "        return best_sequence\n",
        "\n",
        "    Vocabulary = vocab_words\n",
        "    distances = {(word, voc_token): Levenshtein.distance(word, voc_token) for word, voc_token in product(word_list, Vocabulary)}\n",
        "    initial_state = ['<s>', '<s>']  # To be able to correct even the first input word.\n",
        "    max_depth = len(word_list)\n",
        "\n",
        "    best_sequence = beam_search_decode_tri(initial_state, max_depth, beam_width, generate_candidates_tri, score_tri, word_list, distances)\n",
        "\n",
        "    return best_sequence[2:]  # Excluding the \"<start>\" token\n",
        "\n",
        "\n",
        "# Example use:\n",
        "inputText = ['thes', 'is', 'a', 'now', 'yrk', 'citi']  # dianisma W gia na gine auto-corrected\n",
        "print(spellcheck_trigram(inputText))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFbvjV-VAoSX"
      },
      "outputs": [],
      "source": [
        "# %% (v) Generate artificial test data\n",
        "artTestSet = copy.deepcopy(testSet)\n",
        "for i, sentence in enumerate(artTestSet):\n",
        "    for j, word in enumerate(sentence):\n",
        "        if word != 'UNK':\n",
        "            # Break the word into characters. Replace with a small proba each character with a random char\n",
        "            scrambledWord = ''.join([random.choice(string.ascii_letters) if random.random() > 0.9 else letter for letter in word])\n",
        "            artTestSet[i][j] = scrambledWord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozGfYtgNAoSX",
        "outputId": "15cd19bf-bcab-49f6-8625-2fcfb525f2c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inpput text:        ['let', 'af', 'UNK', 'the', 'form', 'of', 'aZ']\n",
            "Bigram spellcheck:  ['let', 'af', 'UNK', 'the', 'form', 'of', 'a']\n",
            "Trigram spellcheck: ['let', 'us', 'UNK', 'the', 'UNK', 'of', 'a']\n"
          ]
        }
      ],
      "source": [
        "# %% (vi) example use\n",
        "inputText = artTestSet[98]\n",
        "print(f\"Inpput text:        {inputText}\")\n",
        "print(f\"Bigram spellcheck:  {spellcheck_bigram(inputText)}\")\n",
        "print(f\"Trigram spellcheck: {spellcheck_trigram(inputText)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yUXXh13AoSY",
        "outputId": "a0b9f363-a256-4183-8945-e1b4eca6dfd1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [06:00<00:00,  3.61s/it]\n",
            "100%|██████████| 100/100 [04:55<00:00,  2.96s/it]\n"
          ]
        }
      ],
      "source": [
        "#%% Spell check the artificial test set\n",
        "to_correct = artTestSet[:100]\n",
        "to_compare = testSet[:100]\n",
        "\n",
        "predictions_bigram = []\n",
        "for sentence in tqdm(to_correct):\n",
        "    corrected_sentence = spellcheck_bigram(sentence)\n",
        "    predictions_bigram.append(corrected_sentence)\n",
        "\n",
        "predictions_trigram = []\n",
        "for sentence in tqdm(to_correct):\n",
        "    corrected_sentence = spellcheck_trigram(sentence)\n",
        "    predictions_trigram.append(corrected_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lULlfVfGAoSY",
        "outputId": "d9c0fbeb-6123-40a9-b5c0-d4cb1254077d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word Error Rate: 0.3087\n",
            "Character Error Rate: 0.2636\n"
          ]
        }
      ],
      "source": [
        "#%% Compute WER CER for bigram\n",
        "references = to_compare\n",
        "\n",
        "def compute_cer(reference, hypothesis):\n",
        "    return Levenshtein.distance(reference, hypothesis) / max(len(reference), len(hypothesis))\n",
        "\n",
        "# Flatten the lists for WER calculation\n",
        "references_flat = [\" \".join(sentence) for sentence in references]\n",
        "predictions_flat = [\" \".join(sentence) for sentence in predictions_bigram]\n",
        "\n",
        "# Compute WER\n",
        "wer = load(\"wer\")\n",
        "wer_score = wer.compute(predictions=predictions_flat, references=references_flat)\n",
        "print(f\"Word Error Rate: {wer_score:.4f}\")\n",
        "\n",
        "# Compute CER\n",
        "cer_scores = [compute_cer(ref, hyp) for ref, hyp in zip(references_flat, predictions_flat)]\n",
        "average_cer = sum(cer_scores) / len(cer_scores)\n",
        "print(f\"Character Error Rate: {average_cer:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q266ODmeAoSZ",
        "outputId": "d764d4e6-8f27-4031-b44a-f2e6799af276"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word Error Rate: 0.4576\n",
            "Character Error Rate: 0.3621\n"
          ]
        }
      ],
      "source": [
        "#%% Compute WER CER for trigram\n",
        "\n",
        "# Flatten the lists for WER calculation\n",
        "predictions_flat = [\" \".join(sentence) for sentence in predictions_trigram]\n",
        "\n",
        "# Compute WER\n",
        "wer = load(\"wer\")\n",
        "wer_score = wer.compute(predictions=predictions_flat, references=references_flat)\n",
        "print(f\"Word Error Rate: {wer_score:.4f}\")\n",
        "\n",
        "# Compute CER\n",
        "cer_scores = [compute_cer(ref, hyp) for ref, hyp in zip(references_flat, predictions_flat)]\n",
        "average_cer = sum(cer_scores) / len(cer_scores)\n",
        "print(f\"Character Error Rate: {average_cer:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5PDTR6eAoSZ"
      },
      "source": [
        "# Examples\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZi4qT7YAoSZ"
      },
      "source": [
        "## Autocomplete:\n",
        "### Notes:\n",
        "- Bigram model can autocomplete almost any sentence but will tend to cycle through the most common bigrams.\n",
        "- Trigram model is more fluent but if the 'seed' bigram is not present the autocompletion breakes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaYjDzqIAoSZ",
        "outputId": "9d7f53d4-1c30-444e-d2a7-605ebd476f87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Autocompleted Bigram:  'I would like a few years ago ,'\n",
            "Autocompleted Trigram: 'I would like to see the car <e>'\n"
          ]
        }
      ],
      "source": [
        "# Example usage:\n",
        "input_text = \"I would like\"\n",
        "print(f\"Autocompleted Bigram:  '{autocomplete_bigram(input_text, max_depth=5, beam_width=2)}'\")\n",
        "print(f\"Autocompleted Trigram: '{autocomplete_trigram(input_text, max_depth=5, beam_width=2)}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmNXTI0YAoSb",
        "outputId": "8cdb8eae-df24-48f8-aae7-03da325e0b21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Autocompleted Bigram:  'The most popular as a few years ago , and the same time , and the same time'\n",
            "Autocompleted Trigram: 'The most popular man on the other hand , the first time in the world , and the'\n"
          ]
        }
      ],
      "source": [
        "input_text = \"The most popular\"\n",
        "print(f\"Autocompleted Bigram:  '{autocomplete_bigram(input_text, max_depth=15, beam_width=2)}'\")\n",
        "print(f\"Autocompleted Trigram: '{autocomplete_trigram(input_text, max_depth=15, beam_width=2)}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fvfu_ZRjAoSb",
        "outputId": "5f6852ce-8b1d-489a-8463-3d72b08b3885"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Autocompleted Bigram:  'In conclusion , and the same time , and the same time , and the same time'\n",
            "Autocompleted Trigram: 'In conclusion <e>'\n"
          ]
        }
      ],
      "source": [
        "input_text = \"In conclusion\"\n",
        "print(f\"Autocompleted Bigram:  '{autocomplete_bigram(input_text, max_depth=15, beam_width=2)}'\")\n",
        "print(f\"Autocompleted Trigram: '{autocomplete_trigram(input_text, max_depth=15, beam_width=2)}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQzwtpIpAoSb",
        "outputId": "df77e6bb-a5fc-4b58-c0ad-f87c8d6c7d8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Autocompleted Bigram:  'federal policies and the same time , and the same time , and the same time ,'\n",
            "Autocompleted Trigram: 'federal policies will produce a better understanding of the united states , and the other hand ,'\n"
          ]
        }
      ],
      "source": [
        "input_text = \"federal policies\"\n",
        "print(f\"Autocompleted Bigram:  '{autocomplete_bigram(input_text, max_depth=15, beam_width=2)}'\")\n",
        "print(f\"Autocompleted Trigram: '{autocomplete_trigram(input_text, max_depth=15, beam_width=2)}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAwpd6FQAoSb"
      },
      "source": [
        "## Context-aware spelling corrector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4E6pLqEAoSc",
        "outputId": "13930090-8320-4ec6-9128-005789764403"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inpput text:        ['i', 'AaJted', 'him', 'p', 'with', 'a', 'UNK', 'UNK']\n",
            "Bigram spellcheck:  ['i', 'have', 'him', ',', 'with', 'a', 'UNK', 'UNK']\n",
            "Trigram spellcheck: ['i', 'asked', 'him', ',', 'with', 'a', 'UNK', 'UNK']\n"
          ]
        }
      ],
      "source": [
        "# %% (vi) example use\n",
        "inputText = artTestSet[91]\n",
        "print(f\"Inpput text:        {inputText}\")\n",
        "print(f\"Bigram spellcheck:  {spellcheck_bigram(inputText)}\")\n",
        "print(f\"Trigram spellcheck: {spellcheck_trigram(inputText)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQ5U4P-dAoSc",
        "outputId": "f79e811b-21e9-47fe-a97c-93676c8ffa31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inpput text:        ['and', 'onc', 'had', 'been', 'too', 'mPny']\n",
            "Bigram spellcheck:  ['and', 'UNK', 'had', 'been', 'too', 'many']\n",
            "Trigram spellcheck: ['and', 'one', 'had', 'been', 'too', 'many']\n"
          ]
        }
      ],
      "source": [
        "# %% (vi) example use\n",
        "inputText = artTestSet[143]\n",
        "print(f\"Inpput text:        {inputText}\")\n",
        "print(f\"Bigram spellcheck:  {spellcheck_bigram(inputText)}\")\n",
        "print(f\"Trigram spellcheck: {spellcheck_trigram(inputText)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJ6B1X2AAoSc",
        "outputId": "76a1a305-03b8-4ada-886d-28027e01392f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inpput text:        ['yoc', 'cyuld', 'wish', 'that']\n",
            "Bigram spellcheck:  ['you', 'could', 'wish', 'that']\n",
            "Trigram spellcheck: ['you', 'could', 'wish', 'for']\n"
          ]
        }
      ],
      "source": [
        "# %% (vi) example use\n",
        "inputText = artTestSet[191]\n",
        "print(f\"Inpput text:        {inputText}\")\n",
        "print(f\"Bigram spellcheck:  {spellcheck_bigram(inputText)}\")\n",
        "print(f\"Trigram spellcheck: {spellcheck_trigram(inputText)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88NxaRC_AoSc",
        "outputId": "7c83b772-64d2-4e09-bebc-ecab788e7f36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inpput text:        ['they', 'walked', 'tAwPrd', 'each', 'otheU']\n",
            "Bigram spellcheck:  ['they', 'walked', 'toward', 'each', 'other']\n",
            "Trigram spellcheck: ['they', 'walked', 'the', 'deck', ',']\n"
          ]
        }
      ],
      "source": [
        "# %% (vi) example use\n",
        "inputText = artTestSet[348]\n",
        "print(f\"Inpput text:        {inputText}\")\n",
        "print(f\"Bigram spellcheck:  {spellcheck_bigram(inputText)}\")\n",
        "print(f\"Trigram spellcheck: {spellcheck_trigram(inputText)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
